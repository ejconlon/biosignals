{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate to chdir to project root and add projects sources to python path\n",
    "import os\n",
    "import sys\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('..')\n",
    "    sys.path.append(os.path.abspath('.'))\n",
    "\n",
    "# Prepare the dataset\n",
    "import biosignals.prepare as bp\n",
    "if not bp.has_prepared('rand'):\n",
    "    bp.prepare_rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate to chdir to project root and add projects sources to python path\n",
    "# ---DELETE---\n",
    "import os\n",
    "import sys\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('..')\n",
    "    sys.path.append(os.path.abspath('.'))\n",
    "\n",
    "import biosignals.deep_models as bdeep\n",
    "# ---DELETE---\n",
    "\n",
    "bdeep.test_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import biosignals.deep_models as bdeep\n",
    "# Test deep learning models (LSTM, GRU) so far\n",
    "bdeep.test_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The rest of this notebook is redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import biosignals.prepare as bp\n",
    "import biosignals.evaluation as be\n",
    "import biosignals.models as bm\n",
    "from biosignals.split import Role\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, GRU, Activation\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared = bp.read_prepared(\"rand\")\n",
    "print(prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepared[Role.TRAIN][0].load()\n",
    "data_val = prepared[Role.VALIDATE][0].load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eric's Random Forest\n",
    "bm.test_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biosignals.models import *\n",
    "# Hacky way to obtain the clustered data by abusing Eric's work...\n",
    "data = bp.read_clusters()\n",
    "model = bm.SkModel(RandomForestClassifier, {}, Strategy.MULTI)\n",
    "lds = bp.read_prepared('rand')\n",
    "data = model._load_all(lds[bs.Role.TRAIN], RandomState(42))\n",
    "data_val = model._load_all(lds[bs.Role.VALIDATE], RandomState(42))\n",
    "data_test = model._load_all(lds[bs.Role.TEST], RandomState(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0].shape, data[1].shape, data_val[0].shape, data_val[1].shape, data_test[0].shape, data_test[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.convert_to_tensor(data[0], dtype=tf.float64)\n",
    "train_labels = tf.convert_to_tensor(data[1], dtype=tf.int32)\n",
    "print(train_labels.shape, train_data.shape)\n",
    "\n",
    "val_data = tf.convert_to_tensor(data_val[0], dtype=tf.float64)\n",
    "val_labels = tf.convert_to_tensor(data_val[1], dtype=tf.int32)\n",
    "\n",
    "test_data = tf.convert_to_tensor(data_test[0], dtype=tf.float64)\n",
    "test_labels = tf.convert_to_tensor(data_test[1], dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard scalar\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data)\n",
    "train_data = scaler.transform(train_data)\n",
    "val_data = scaler.transform(val_data)\n",
    "test_data = scaler.transform(test_data)\n",
    "\n",
    "#PCA\n",
    "n_components = 64\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(train_data)\n",
    "train_data_pca = pca.transform(train_data)\n",
    "val_data_pca = pca.transform(val_data)\n",
    "test_data_pca = pca.transform(test_data)\n",
    "\n",
    "\n",
    "# Expand dimensions\n",
    "train_data_pca = tf.expand_dims(train_data_pca, axis=1)\n",
    "val_data_pca = tf.expand_dims(val_data_pca, axis=1)\n",
    "test_data_pca = tf.expand_dims(test_data_pca, axis=1)\n",
    "\n",
    "train_data_nopca = tf.expand_dims(train_data, axis=1)\n",
    "val_data_nopca = tf.expand_dims(val_data, axis=1)\n",
    "test_data_nopca = tf.expand_dims(test_data, axis=1)\n",
    "\n",
    "print(train_data_pca.shape, val_data_pca.shape, test_data_pca.shape)\n",
    "print(train_data_nopca.shape, val_data_nopca.shape, test_data_nopca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU NO PCA\n",
    "model_GRU_nopca = bdeep.GRU_Model(train_data_nopca, train_labels, num_epochs=30, batch_size=64)\n",
    "be.evaluate_model(model_GRU_nopca.predict(val_data_nopca), val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU WITH PCA\n",
    "model_GRU_pca = bdeep.GRU_Model(train_data_pca, train_labels, num_epochs=30, batch_size=64)\n",
    "be.evaluate_model(model_GRU_pca.predict(val_data_pca), val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM NO PCA\n",
    "model_LSTM_nopca = bdeep.LSTM_Model(train_data_nopca, train_labels, num_epochs=30, batch_size=64)\n",
    "be.evaluate_model(model_LSTM_nopca.predict(val_data_nopca), val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM WITH PCA\n",
    "model_LSTM_pca = bdeep.LSTM_Model(train_data_pca, train_labels, num_epochs=30, batch_size=64)\n",
    "be.evaluate_model(model_LSTM_pca.predict(val_data_pca), val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation\n",
    "print(\"GRU - no PCA\")\n",
    "be.evaluate_model(model_GRU_nopca.predict(test_data_nopca), test_labels)\n",
    "print(\"GRU - with PCA\")\n",
    "be.evaluate_model(model_GRU_pca.predict(test_data_pca), test_labels)\n",
    "\n",
    "print(\"LSTM - no PCA\")\n",
    "be.evaluate_model(model_LSTM_nopca.predict(test_data_nopca), test_labels)\n",
    "print(\"LSTM - with PCA\")\n",
    "be.evaluate_model(model_LSTM_pca.predict(test_data_pca), test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training evaluation (to see if we overfit)\n",
    "print(\"GRU - no PCA\")\n",
    "be.evaluate_model(model_GRU_nopca.predict(train_data_nopca), train_labels)\n",
    "print(\"GRU - PCA\")\n",
    "be.evaluate_model(model_GRU_pca.predict(train_data_pca), train_labels)\n",
    "print(\"LSTM - no PCA\")\n",
    "be.evaluate_model(model_LSTM_nopca.predict(train_data_nopca), train_labels)\n",
    "print(\"LSTM - PCA\")\n",
    "be.evaluate_model(model_LSTM_pca.predict(train_data_pca), train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2094369740ab7c88433fe3a51e1e4e840ab1460a13164a5fa7d1926c1d19676f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
